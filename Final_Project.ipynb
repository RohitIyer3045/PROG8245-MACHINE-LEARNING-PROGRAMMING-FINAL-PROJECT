{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233d283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Collection, Tokenizer, Normalization Pipeline on IMDB dataset\n",
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import logging\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f577cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "aca606e6-18fc-4e8b-9cbc-57fcc699fbbb",
       "rows": [
        [
         "0",
         "Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tones) becomes acquainted and friends with a fourteen-year-old boy from Wisconsin named Pete Logand(Rory Culkin),who has written a book detailing sexual abuse from his parents. To boot,Pete has AIDS and this compels Gabriel further still,since his partner Jess(Bobby Cannavale,good)happens to be a survivor of HIV himself. <br /><br />He also acquaints himself with Pete's guardian,a woman named Donna(Toni Collette,brilliant!)and when Gabriel decides he wants to meet and talk to the two of them in person and goes to Wisconsin,he discovers some secrets he was(naturally)not prepared to find.<br /><br />Based on real events that happened to Armistead Maupin(who co-wrote the screenplay with Terry Anderson)and directed by Patrick Stetner,this film moves a lot faster(90 min.,maybe a few minutes longer)than one might think a movie of this genre would run. That's good in that it keeps the action and storyline lean and clear. It's bad in that it leaves various holes in the plot and doesn't sew-up any of the plot openings or back-story. I'd rather not go into any great detail except to say that,if you are not familiar with Mr.Maupin's works or his personal story,you feel a little bit out of the loop here. Still,the performances by Williams( I would've loved to heard more of his narration,personally),Collette,Cannavale,Culkin and much of the supporting cast(the Waitress at the restaurant Collete's Donna frequents does a great job with what small part she has!)are top-notch and the mood established here--namely,the chilly,lonely dark exteriors of Wisconsin and New York--give a terrific framing for this story. It may have ends that don't tie together particularly well,but it's still a compelling enough story to stick with.",
         "positive"
        ],
        [
         "1",
         "Stuck in a hotel in Kuwait, I happily switched to the channel showing this at the very beginning. First Pachelbel's Canon brought a lump to my throat, then the sight of a Tiger Moth (which my grandfather, my father and I have all flown) produced a slight dampness around the eyes and then Crowe's name hooked me completely. I was entranced by this film, Crowe's performance (again), the subject matter (and yes, what a debt we owe), how various matters were addressed and dealt with, the flying sequences (my father flew Avro Ansons, too), the story - and, as another contributor pointed out, Crowe's recitation of High Flight. I won't spoil the film for anyone, but, separated from my wife by 4,000-odd miles, as an ex-army officer who was deployed in a couple of wars and as private pilot, I admit to crying heartily a couple of times. Buy it, rent it, download it, beg, borrow or steal it - but watch it.<br /><br />PS Did I spy a Bristol Blenheim (in yellow training colours)on the ground? Looked like a twin-engine aircraft with a twin-.303 Brownings in a dorsal turret.",
         "positive"
        ],
        [
         "2",
         "This movie is BAD! It's basically an overdone copy of Michael Jackson's Thriller video, only worse! The special effects consist of lots of glow in the dark paint, freaky slapstick fastmoving camera shots and lots of growling. I think the dog was the best actor in the whole movie.",
         "negative"
        ],
        [
         "3",
         "As long as you go into this movie knowing that it's terrible: bad acting, bad \"effects,\" bad story, bad... everything, then you'll love it. This is one of my favorite \"goof on\" movies; watch it as a comedy and have a dozen good laughs!",
         "positive"
        ],
        [
         "4",
         "On the way back from IMC6 (San Jose, California), all five (mind you, three of us hardcore Kamal fans) of us had reached a unanimous verdict; VV was solid crap and thanks to the movie we were going to have a pretty screwed up Monday. Not to mention, we swore to stay off the theatres for the next year.<br /><br />I won't blame Kamal here because he sort of dropped a hint in a recent interview with cartoonist Madan (on Vijay TV). He said something like, \"Tamizh Cinema'la Photography, Editing'la namba munnera'na maadri Screenplay, Direction, Acting'la innum namba munnera'la\" (Tamil Cinema has grown in terms of Photography and Editing, but we have hardly improved, when it comes to Screenplay, Direction and Acting\"). While you're watching VV, those words ring very true.<br /><br />Now, here are the 10 Reasons to hate this movie:<br /><br />1. Harris Jeyaraj<br /><br />2. Harris Jeyaraj<br /><br />3. Harris Jeyaraj I'm barely holding myself from using expletives here, but fact is HJ has mastered the fine knack of screwing up every recent movie of his (remember 'Anniyan', 'Ghajini') with the jarring cacophony, he bills as background music. The next time I have an eardrum transplant, he's paying for it. <br /><br />4. Songs Neither do the songs help move the movie's narration spatially/temporally nor do they make you sit up and take notice. The film feels like it's made of four VERY long songs with a few scenes thrown in between them.<br /><br />5. A Short gone too far. VV at best is fit to be a short story, not a 2 hour plus \"thriller\". To use a cliché here, like the Energizer bunny it goes on and on and on; only in this case you don't want it to. The later part of a movie feels like a big drag.<br /><br />6. Kamal-Jothika pairing Two ice cubes rubbed together could've produced more sparks than this lead pairing. There's no reason you would root for them to make it together. In fact every time they get together in the second half of the movie, they make a good irritant to the narration. Hate to say this, but Kamalini Mukerjhee's 10 minute romancing does more than what Kamal and Jothika achieve in this movie plus 'Thenali'.<br /><br />7. Kamal Haasan's accent Kamal has this pretentious accent that nobody speaks either in India or in the US; and it isn't new either. He's been doing it since 'Thoongadae Thambi Thoongadae'. It's simply gets on the nerve. Imagine what havoc it can cause when his flair for using this strange accent meets shooting on location in the US. He doesn't leave it at the Immigration either, he offers doses of advice to his men (bewildered TN Cops from Keeranor, Sathoor and beyond) in chaste Kamanglish (\"Wha we hav here is plain bad police wok\"), of course with nauseating effect.<br /><br />8. Logic There are a few directors whom you expect to stand up to a certain scale. Gautam fails us badly with some crappy performance in the Department of common sense. Which D.C.P in his senses would meet his love interest on the streets to discuss such matters as committing himself and life after! The scene inside the theatre was so bad, towards the climax; we could hear people behind us loudly challenge the Hero's IQ. \"Is he stupid, can't he just use his Siren or Lights?\" (On a busy Madras road, Kamal-the-cop-on-a-police-Jeep chases a guy on a bike just like any ordinary dude!). \"Can't he just use his gun?\" (\"The guy on a bike\" starts on foot and we have a fully geared Kamal in hot pursuit for a considerable amount of time). I'm not voting in favour of the later, but I'm just trying to explain the mood inside.<br /><br />9. Gore & Violence If I wanted to watch women being raped, their throats getting slashed, more women getting raped and thrown into the bushes with excruciating authenticity, I would sit at home and rather watch a \"Police Report\" or \"Kuttram\". The use of excessive violence should go in a way to extend the story, not overwhelm it! Somewhere down the line Gautum seems confused about what the extensions (rapes, murders) are and what the mainstay (story) is!<br /><br />10. Even a double shot Espresso couldn't get the pain out of the head.",
         "negative"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Popular radio storyteller Gabriel No one(Robin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stuck in a hotel in Kuwait, I happily switched...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is BAD! It's basically an overdone ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As long as you go into this movie knowing that...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the way back from IMC6 (San Jose, Californi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Popular radio storyteller Gabriel No one(Robin...  positive\n",
       "1  Stuck in a hotel in Kuwait, I happily switched...  positive\n",
       "2  This movie is BAD! It's basically an overdone ...  negative\n",
       "3  As long as you go into this movie knowing that...  positive\n",
       "4  On the way back from IMC6 (San Jose, Californi...  negative"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 1: Download IMDB dataset (2000 samples: 1000 pos, 1000 neg) ---\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset_path = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    import requests\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(dataset_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# Extract only once\n",
    "if not os.path.exists(\"aclImdb\"):\n",
    "    with tarfile.open(dataset_path, \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "# --- Step 2: Collect 1000 pos + 1000 neg ---\n",
    "base = \"aclImdb/train\"\n",
    "\n",
    "pos_files = os.listdir(f\"{base}/pos\")[:1000]\n",
    "neg_files = os.listdir(f\"{base}/neg\")[:1000]\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Load positive reviews\n",
    "for fname in pos_files:\n",
    "    with open(f\"{base}/pos/{fname}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        documents.append((f.read(), \"positive\"))\n",
    "\n",
    "# Load negative reviews\n",
    "for fname in neg_files:\n",
    "    with open(f\"{base}/neg/{fname}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        documents.append((f.read(), \"negative\"))\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(documents, columns=['text', 'label'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a938b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2000, 2)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "positive    1000\n",
      "negative    1000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "cd26d04e-81ea-4302-9cea-69aa14d252eb",
       "rows": [
        [
         "0",
         "Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tones) becomes acquainted and friends with a fourteen-year-old boy from Wisconsin named Pete Logand(Rory Culkin),who has written a book detailing sexual abuse from his parents. To boot,Pete has AIDS and this compels Gabriel further still,since his partner Jess(Bobby Cannavale,good)happens to be a survivor of HIV himself. <br /><br />He also acquaints himself with Pete's guardian,a woman named Donna(Toni Collette,brilliant!)and when Gabriel decides he wants to meet and talk to the two of them in person and goes to Wisconsin,he discovers some secrets he was(naturally)not prepared to find.<br /><br />Based on real events that happened to Armistead Maupin(who co-wrote the screenplay with Terry Anderson)and directed by Patrick Stetner,this film moves a lot faster(90 min.,maybe a few minutes longer)than one might think a movie of this genre would run. That's good in that it keeps the action and storyline lean and clear. It's bad in that it leaves various holes in the plot and doesn't sew-up any of the plot openings or back-story. I'd rather not go into any great detail except to say that,if you are not familiar with Mr.Maupin's works or his personal story,you feel a little bit out of the loop here. Still,the performances by Williams( I would've loved to heard more of his narration,personally),Collette,Cannavale,Culkin and much of the supporting cast(the Waitress at the restaurant Collete's Donna frequents does a great job with what small part she has!)are top-notch and the mood established here--namely,the chilly,lonely dark exteriors of Wisconsin and New York--give a terrific framing for this story. It may have ends that don't tie together particularly well,but it's still a compelling enough story to stick with.",
         "positive"
        ],
        [
         "1",
         "Stuck in a hotel in Kuwait, I happily switched to the channel showing this at the very beginning. First Pachelbel's Canon brought a lump to my throat, then the sight of a Tiger Moth (which my grandfather, my father and I have all flown) produced a slight dampness around the eyes and then Crowe's name hooked me completely. I was entranced by this film, Crowe's performance (again), the subject matter (and yes, what a debt we owe), how various matters were addressed and dealt with, the flying sequences (my father flew Avro Ansons, too), the story - and, as another contributor pointed out, Crowe's recitation of High Flight. I won't spoil the film for anyone, but, separated from my wife by 4,000-odd miles, as an ex-army officer who was deployed in a couple of wars and as private pilot, I admit to crying heartily a couple of times. Buy it, rent it, download it, beg, borrow or steal it - but watch it.<br /><br />PS Did I spy a Bristol Blenheim (in yellow training colours)on the ground? Looked like a twin-engine aircraft with a twin-.303 Brownings in a dorsal turret.",
         "positive"
        ],
        [
         "2",
         "This movie is BAD! It's basically an overdone copy of Michael Jackson's Thriller video, only worse! The special effects consist of lots of glow in the dark paint, freaky slapstick fastmoving camera shots and lots of growling. I think the dog was the best actor in the whole movie.",
         "negative"
        ],
        [
         "3",
         "As long as you go into this movie knowing that it's terrible: bad acting, bad \"effects,\" bad story, bad... everything, then you'll love it. This is one of my favorite \"goof on\" movies; watch it as a comedy and have a dozen good laughs!",
         "positive"
        ],
        [
         "4",
         "On the way back from IMC6 (San Jose, California), all five (mind you, three of us hardcore Kamal fans) of us had reached a unanimous verdict; VV was solid crap and thanks to the movie we were going to have a pretty screwed up Monday. Not to mention, we swore to stay off the theatres for the next year.<br /><br />I won't blame Kamal here because he sort of dropped a hint in a recent interview with cartoonist Madan (on Vijay TV). He said something like, \"Tamizh Cinema'la Photography, Editing'la namba munnera'na maadri Screenplay, Direction, Acting'la innum namba munnera'la\" (Tamil Cinema has grown in terms of Photography and Editing, but we have hardly improved, when it comes to Screenplay, Direction and Acting\"). While you're watching VV, those words ring very true.<br /><br />Now, here are the 10 Reasons to hate this movie:<br /><br />1. Harris Jeyaraj<br /><br />2. Harris Jeyaraj<br /><br />3. Harris Jeyaraj I'm barely holding myself from using expletives here, but fact is HJ has mastered the fine knack of screwing up every recent movie of his (remember 'Anniyan', 'Ghajini') with the jarring cacophony, he bills as background music. The next time I have an eardrum transplant, he's paying for it. <br /><br />4. Songs Neither do the songs help move the movie's narration spatially/temporally nor do they make you sit up and take notice. The film feels like it's made of four VERY long songs with a few scenes thrown in between them.<br /><br />5. A Short gone too far. VV at best is fit to be a short story, not a 2 hour plus \"thriller\". To use a cliché here, like the Energizer bunny it goes on and on and on; only in this case you don't want it to. The later part of a movie feels like a big drag.<br /><br />6. Kamal-Jothika pairing Two ice cubes rubbed together could've produced more sparks than this lead pairing. There's no reason you would root for them to make it together. In fact every time they get together in the second half of the movie, they make a good irritant to the narration. Hate to say this, but Kamalini Mukerjhee's 10 minute romancing does more than what Kamal and Jothika achieve in this movie plus 'Thenali'.<br /><br />7. Kamal Haasan's accent Kamal has this pretentious accent that nobody speaks either in India or in the US; and it isn't new either. He's been doing it since 'Thoongadae Thambi Thoongadae'. It's simply gets on the nerve. Imagine what havoc it can cause when his flair for using this strange accent meets shooting on location in the US. He doesn't leave it at the Immigration either, he offers doses of advice to his men (bewildered TN Cops from Keeranor, Sathoor and beyond) in chaste Kamanglish (\"Wha we hav here is plain bad police wok\"), of course with nauseating effect.<br /><br />8. Logic There are a few directors whom you expect to stand up to a certain scale. Gautam fails us badly with some crappy performance in the Department of common sense. Which D.C.P in his senses would meet his love interest on the streets to discuss such matters as committing himself and life after! The scene inside the theatre was so bad, towards the climax; we could hear people behind us loudly challenge the Hero's IQ. \"Is he stupid, can't he just use his Siren or Lights?\" (On a busy Madras road, Kamal-the-cop-on-a-police-Jeep chases a guy on a bike just like any ordinary dude!). \"Can't he just use his gun?\" (\"The guy on a bike\" starts on foot and we have a fully geared Kamal in hot pursuit for a considerable amount of time). I'm not voting in favour of the later, but I'm just trying to explain the mood inside.<br /><br />9. Gore & Violence If I wanted to watch women being raped, their throats getting slashed, more women getting raped and thrown into the bushes with excruciating authenticity, I would sit at home and rather watch a \"Police Report\" or \"Kuttram\". The use of excessive violence should go in a way to extend the story, not overwhelm it! Somewhere down the line Gautum seems confused about what the extensions (rapes, murders) are and what the mainstay (story) is!<br /><br />10. Even a double shot Espresso couldn't get the pain out of the head.",
         "negative"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Popular radio storyteller Gabriel No one(Robin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stuck in a hotel in Kuwait, I happily switched...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is BAD! It's basically an overdone ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As long as you go into this movie knowing that...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the way back from IMC6 (San Jose, Californi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Popular radio storyteller Gabriel No one(Robin...  positive\n",
       "1  Stuck in a hotel in Kuwait, I happily switched...  positive\n",
       "2  This movie is BAD! It's basically an overdone ...  negative\n",
       "3  As long as you go into this movie knowing that...  positive\n",
       "4  On the way back from IMC6 (San Jose, Californi...  negative"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "df[['text', 'label']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba030466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST REVIEW (text sample) ===\n",
      "Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tones) becomes acquainted and friends with a fourteen-year-old boy from Wisconsin named Pete Logand(Rory Culkin),who has written a book detailing sexual abuse from his parents. To boot,Pete has AIDS and t ...\n",
      "\n",
      "=== TOKENIZED OUTPUT ===\n",
      "['Popular', 'radio', 'storyteller', 'Gabriel', 'No', 'one', '(', 'Robin', 'Williams', ',', 'scraggy', 'and', 'speaking', 'in', 'hushed', ',', 'hypnotic', 'tones', ')', 'becomes', 'acquainted', 'and', 'friends', 'with', 'a', 'fourteen-year-old', 'boy', 'from', 'Wisconsin', 'named']\n"
     ]
    }
   ],
   "source": [
    "first_review = df.loc[0, \"text\"]\n",
    "first_review_tokens = word_tokenize(first_review)\n",
    "\n",
    "print(\"=== FIRST REVIEW (text sample) ===\")\n",
    "print(first_review[:300], \"...\")\n",
    "\n",
    "print(\"\\n=== TOKENIZED OUTPUT ===\")\n",
    "print(first_review_tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac956c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NORMALIZATION & STOP WORDS REMOVAL ---\n",
      "['popular', 'radio', 'storyteller', 'gabriel', 'one', '(', 'robin', 'williams', ',', 'scraggy', 'speaking', 'hushed', ',', 'hypnotic', 'tones', ')', 'becomes', 'acquainted', 'friends', 'fourteen-year-old', 'boy', 'wisconsin', 'named', 'pete', 'logand', '(', 'rory', 'culkin', ')', ',']\n"
     ]
    }
   ],
   "source": [
    "def normalize_and_remove_stops(tokens):\n",
    "    \"\"\"\n",
    "    1. Converts all tokens to lowercase (Normalization).\n",
    "    2. Removes common English stop words.\n",
    "    \"\"\"\n",
    "    # 1. Normalization: Convert to lowercase\n",
    "    normalized_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # 2. Stop Words Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in normalized_tokens if token not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Process our IMDB review tokens\n",
    "cleaned_tokens = normalize_and_remove_stops(first_review_tokens)\n",
    "\n",
    "print(\"--- NORMALIZATION & STOP WORDS REMOVAL ---\")\n",
    "print(cleaned_tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92c2d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1500\n",
      "Test size : 500\n"
     ]
    }
   ],
   "source": [
    "X = df[\"text\"].values          # raw review text\n",
    "y = df[\"label\"].values         # \"positive\" / \"negative\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size :\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08dfcbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF MATRIX INFO ===\n",
      "Train TF-IDF shape: (1500, 5000)\n",
      "Test TF-IDF shape : (500, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # limit vocabulary size for practicality\n",
    "    ngram_range=(1, 1),     # unigrams only for now\n",
    "    lowercase=True,         # convert to lowercase\n",
    "    stop_words='english'    # remove English stopwords\n",
    ")\n",
    "\n",
    "# Fit on training data only (to avoid data leakage)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"\\n=== TF-IDF MATRIX INFO ===\")\n",
    "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
    "print(\"Test TF-IDF shape :\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cc15e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 5000\n",
      "\n",
      "Sample of first 100 features:\n",
      "['00' '000' '10' '100' '11' '11th' '12' '13' '14' '15' '16' '17' '18'\n",
      " '1912' '1930' '1930s' '1933' '1939' '1940' '1943' '1949' '1950' '1960'\n",
      " '1963' '1968' '1970' '1970s' '1976' '1980' '1986' '1987' '1989' '1990'\n",
      " '1993' '1994' '1996' '1997' '1999' '1st' '20' '2000' '2001' '2002' '2004'\n",
      " '2005' '2006' '2007' '2008' '22' '24' '25' '2nd' '30' '305' '30s' '35'\n",
      " '3d' '3rd' '40' '40s' '42nd' '45' '50' '50s' '60' '64' '70' '70s' '75'\n",
      " '77' '80' '80s' '85' '90' '90s' '95' '99' 'abandoned' 'abbey' 'abby'\n",
      " 'abilities' 'ability' 'able' 'aboard' 'abroad' 'absent' 'absolute'\n",
      " 'absolutely' 'absorbing' 'absurd' 'abu' 'abundance' 'abuse' 'academy'\n",
      " 'accent' 'accents' 'accept' 'acceptable' 'accepted' 'accepting']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"\\nVocabulary size:\", len(vocab))\n",
    "\n",
    "print(\"\\nSample of first 100 features:\")\n",
    "print(vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adcc856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP 10 TERMS BY TF-IDF WEIGHT (DOC 0) ===\n",
      "Original review snippet:\n",
      "so... it's really sexist, and classist, and i thought that it might not be in the beginning stages of the movie, like when stella tells steven that she would really like to change herself and begin speaking in the right way and he tells her not to change. well, he certainly changed his tune, and it  ...\n",
      "\n",
      "Top 10 weighted terms:\n",
      "social               -> 0.3162\n",
      "sad                  -> 0.2735\n",
      "hero                 -> 0.2680\n",
      "tells                -> 0.2667\n",
      "change               -> 0.2539\n",
      "beginning            -> 0.2360\n",
      "really               -> 0.1992\n",
      "stella               -> 0.1939\n",
      "stages               -> 0.1893\n",
      "makes                -> 0.1799\n"
     ]
    }
   ],
   "source": [
    "doc_index = 0  # first training document\n",
    "doc_vector = X_train_tfidf[doc_index].toarray().ravel()\n",
    "\n",
    "# Indices of top 10 highest-weighted terms in this document\n",
    "top_indices = doc_vector.argsort()[::-1][:10]\n",
    "\n",
    "print(\"\\n=== TOP 10 TERMS BY TF-IDF WEIGHT (DOC 0) ===\")\n",
    "print(\"Original review snippet:\")\n",
    "print(X_train[doc_index][:300], \"...\")\n",
    "\n",
    "print(\"\\nTop 10 weighted terms:\")\n",
    "for idx in top_indices:\n",
    "    term = vocab[idx]\n",
    "    weight = doc_vector[idx]\n",
    "    print(f\"{term:<20} -> {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7de3494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SPARSITY INFORMATION (TRAIN MATRIX) ===\n",
      "Non-zero entries: 103764\n",
      "Total elements  : 7500000\n",
      "Sparsity        : 1.3835%\n",
      "\n",
      "=== SMALL DENSE VIEW (5 docs x 10 features) ===\n",
      "    00  000      10  100   11  11th   12   13   14   15\n",
      "0  0.0  0.0  0.0000  0.0  0.0   0.0  0.0  0.0  0.0  0.0\n",
      "1  0.0  0.0  0.0000  0.0  0.0   0.0  0.0  0.0  0.0  0.0\n",
      "2  0.0  0.0  0.0000  0.0  0.0   0.0  0.0  0.0  0.0  0.0\n",
      "3  0.0  0.0  0.0788  0.0  0.0   0.0  0.0  0.0  0.0  0.0\n",
      "4  0.0  0.0  0.0000  0.0  0.0   0.0  0.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "total_elements = X_train_tfidf.shape[0] * X_train_tfidf.shape[1]\n",
    "non_zero = X_train_tfidf.nnz\n",
    "sparsity = 100.0 * non_zero / total_elements\n",
    "\n",
    "print(\"\\n=== SPARSITY INFORMATION (TRAIN MATRIX) ===\")\n",
    "print(f\"Non-zero entries: {non_zero}\")\n",
    "print(f\"Total elements  : {total_elements}\")\n",
    "print(f\"Sparsity        : {sparsity:.4f}%\")\n",
    "\n",
    "# Show a tiny dense sample of the TF-IDF matrix (first 5 docs, first 10 features)\n",
    "sample_dense = X_train_tfidf[:5, :10].toarray()\n",
    "sample_features = vocab[:10]\n",
    "\n",
    "print(\"\\n=== SMALL DENSE VIEW (5 docs x 10 features) ===\")\n",
    "sample_df = pd.DataFrame(sample_dense, columns=sample_features)\n",
    "print(sample_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: TF-IDF Feature Extraction on IMDB Reviews\n",
    "nltk.download('punkt')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# We assume df from Step 1 exists with columns: [\"text\", \"label\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Train-Test Split\n",
    "# -----------------------------\n",
    "X = df[\"text\"].values          # raw review text\n",
    "y = df[\"label\"].values         # \"positive\" / \"negative\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size :\", len(X_test))\n",
    "\n",
    "# -----------------------------\n",
    "# 2. TF-IDF Vectorization\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "TF-IDF (Term Frequency–Inverse Document Frequency) does:\n",
    "- Term Frequency (TF): how often a word appears in a document.\n",
    "- Inverse Document Frequency (IDF): down-weights words that appear in many documents.\n",
    "Result: words that are frequent in a document but rare across the corpus get higher weights.\n",
    "\"\"\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # limit vocabulary size for practicality\n",
    "    ngram_range=(1, 1),     # unigrams only for now\n",
    "    lowercase=True,         # convert to lowercase\n",
    "    stop_words='english'    # remove English stopwords\n",
    ")\n",
    "\n",
    "# Fit on training data only (to avoid data leakage)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"\\n=== TF-IDF MATRIX INFO ===\")\n",
    "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
    "print(\"Test TF-IDF shape :\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07059725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Vocabulary Inspection\n",
    "# -----------------------------\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"\\nVocabulary size:\", len(vocab))\n",
    "\n",
    "print(\"\\nSample of first 50 features:\")\n",
    "print(vocab[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Inspect TF-IDF Weights for a Single Review\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "\n",
    "doc_index = 0  # first training document\n",
    "doc_vector = X_train_tfidf[doc_index].toarray().ravel()\n",
    "\n",
    "# Indices of top 10 highest-weighted terms in this document\n",
    "top_indices = doc_vector.argsort()[::-1][:10]\n",
    "\n",
    "print(\"\\n=== TOP 10 TERMS BY TF-IDF WEIGHT (DOC 0) ===\")\n",
    "print(\"Original review snippet:\")\n",
    "print(X_train[doc_index][:300], \"...\")\n",
    "\n",
    "print(\"\\nTop 10 weighted terms:\")\n",
    "for idx in top_indices:\n",
    "    term = vocab[idx]\n",
    "    weight = doc_vector[idx]\n",
    "    print(f\"{term:<20} -> {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Visualizing TF-IDF Matrix Structure\n",
    "# -----------------------------\n",
    "total_elements = X_train_tfidf.shape[0] * X_train_tfidf.shape[1]\n",
    "non_zero = X_train_tfidf.nnz\n",
    "sparsity = 100.0 * non_zero / total_elements\n",
    "\n",
    "print(\"\\n=== SPARSITY INFORMATION (TRAIN MATRIX) ===\")\n",
    "print(f\"Non-zero entries: {non_zero}\")\n",
    "print(f\"Total elements  : {total_elements}\")\n",
    "print(f\"Sparsity        : {sparsity:.4f}%\")\n",
    "\n",
    "# Show a tiny dense sample of the TF-IDF matrix (first 5 docs, first 10 features)\n",
    "sample_dense = X_train_tfidf[:5, :10].toarray()\n",
    "sample_features = vocab[:10]\n",
    "\n",
    "print(\"\\n=== SMALL DENSE VIEW (5 docs x 10 features) ===\")\n",
    "sample_df = pd.DataFrame(sample_dense, columns=sample_features)\n",
    "print(sample_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model - Naive Bayes with TF-IDF\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We assume:\n",
    "# X_train_tfidf, X_test_tfidf, y_train, y_test, vectorizer already exist.\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Train Naive Bayes Classifier\n",
    "# -----------------------------\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Predictions\n",
    "# -----------------------------\n",
    "\n",
    "y_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n=== SAMPLE PREDICTIONS ===\")\n",
    "for i in range(5):\n",
    "    print(f\"Review {i}: Predicted={y_pred[i]}, Actual={y_test[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee57d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Confusion Matrix\n",
    "# -----------------------------\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[\"positive\", \"negative\"])\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[\"Actual_Positive\", \"Actual_Negative\"],\n",
    "                     columns=[\"Pred_Positive\", \"Pred_Negative\"])\n",
    "\n",
    "print(\"\\n=== CONFUSION MATRIX ===\")\n",
    "print(cm_df)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Naive Bayes – Confusion Matrix (IMDB Reviews)\")\n",
    "plt.show()\n",
    "\n",
    "# Extract values\n",
    "TP = cm[0,0]\n",
    "FN = cm[0,1]\n",
    "FP = cm[1,0]\n",
    "TN = cm[1,1]\n",
    "\n",
    "print(f\"\\nTP={TP}, FN={FN}, FP={FP}, TN={TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69237899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Evaluation Metrics\n",
    "# -----------------------------\n",
    "\n",
    "accuracy  = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=\"positive\")\n",
    "recall    = recall_score(y_test, y_pred, pos_label=\"positive\")\n",
    "f1        = f1_score(y_test, y_pred, pos_label=\"positive\")\n",
    "\n",
    "print(\"\\n=== METRICS ===\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604535cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Interpretation of Naive Bayes\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\n=== INTERPRETATION OF RESULTS ===\")\n",
    "\n",
    "if FN > FP:\n",
    "    print(\"- The model is making more **False Negatives** (positive reviews predicted as negative).\")\n",
    "    print(\"  → This means the classifier struggles to detect subtle positive sentiment.\")\n",
    "    \n",
    "if FP > FN:\n",
    "    print(\"- The model is making more **False Positives** (negative reviews predicted as positive).\")\n",
    "    print(\"  → The model is confused by emotionally strong negative words that appear in positive contexts.\")\n",
    "\n",
    "print(\"- High precision means predictions marked as 'positive' are usually correct.\")\n",
    "print(\"- High recall means the model can successfully find most positive reviews.\")\n",
    "print(\"- A gap between precision and recall indicates bias toward one class.\")\n",
    "\n",
    "print(\"\\nGeneral Observations:\")\n",
    "print(\"* Naive Bayes works well with TF-IDF because it assumes word independence.\")\n",
    "print(\"* However, it may misclassify reviews with mixed sentiment (e.g., 'The movie was good but slow').\")\n",
    "print(\"* It also struggles with sarcasm, irony, and highly figurative language.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
